{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp vision.gan.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from IPython.display import clear_output\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import logging\n",
    "import functools\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from dotenv import load_dotenv\n",
    "from more_itertools import pairwise\n",
    "from collections import OrderedDict\n",
    "from practical_ai.layers import Identity\n",
    "from practical_ai.data import get_dataset, get_data_loader\n",
    "from practical_ai.vision.gan.loss import get_adversarial_losses_fns\n",
    "from practical_ai.vision.gan.hparams import (\n",
    "    # dataset\n",
    "    DATASET,\n",
    "    LATENT_DIM,\n",
    "    DIM,\n",
    "    CHANNELS,\n",
    "    # architecture, loss\n",
    "    GENERATOR_TYPE,\n",
    "    DISCRIMINATOR_TYPE,\n",
    "    ADVERSARIAL_LOSS_TYPE,\n",
    "    NORM_TYPE,\n",
    "    DIM_CHANNEL_MULTIPLIER,\n",
    "    KERNEL_SIZE,\n",
    "    # training\n",
    "    BATCH_SIZE,\n",
    "    LR,\n",
    "    BETA1,\n",
    "    BETA2,\n",
    ") \n",
    "\n",
    "\n",
    "_ = load_dotenv()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vision.gan.core\n",
    "\n",
    "> 對抗生成網路（Generative Adversarial Network）的核心模組，定義基本的生成器、辨別器以及 GAN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_n_samplings(dim):\n",
    "    return int(torch.log2(torch.tensor(dim, dtype=torch.float32)).item()) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 32\n",
    "assert get_n_samplings(32) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_norm2d(name):\n",
    "    if name == \"identity\":\n",
    "        return Identity\n",
    "    elif name == \"batch\":\n",
    "        return nn.BatchNorm2d\n",
    "    elif name == \"instance\":\n",
    "        return functools.partial(nn.InstanceNorm2d, affine=True)\n",
    "    elif name == \"layer\":\n",
    "        return lambda num_features: nn.GroupNorm(1, num_features)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "practical_ai.layers.Identity"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = get_norm2d(\"identity\")\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_activation(name):\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif name == \"leaky_relu\":\n",
    "        return nn.LeakyReLU(0.2)\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = get_activation(\"relu\")\n",
    "act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UpsampleConv2d(nn.Sequential):\n",
    "    \"\"\"基本上採樣： ConvTransponse2d -> Norm -> Activation\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 stride=2,\n",
    "                 padding=1,\n",
    "                 norm_type=\"batch\",\n",
    "                 act=\"relu\",\n",
    "                 bias=True):\n",
    "        \n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, \n",
    "                               out_channels, \n",
    "                               kernel_size, \n",
    "                               stride, \n",
    "                               padding,\n",
    "                               bias=bias)]\n",
    "        \n",
    "        if norm_type != \"none\":\n",
    "            layers.append(get_norm2d(norm_type)(out_channels))\n",
    "        \n",
    "        if act not in [\"none\", \"linear\"]:\n",
    "            layers.append(get_activation(act))\n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般使用： Transposed Conv -> Norm -> Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpsampleConv2d(\n",
       "  (0): ConvTranspose2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 3\n",
    "out_channels = 128\n",
    "h = w = 32\n",
    "batch_size = 8\n",
    "\n",
    "x = torch.randn(batch_size, in_channels, h, w)\n",
    "upconv = UpsampleConv2d(in_channels, out_channels)\n",
    "out = upconv(x)\n",
    "\n",
    "assert len(upconv) == 3\n",
    "assert out.shape == (batch_size, out_channels, h * 2, w * 2)\n",
    "upconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "單純上採樣，不使用 Norm 以及非線性 activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UpsampleConv2d(\n",
      "  (0): ConvTranspose2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      ")\n",
      "UpsampleConv2d(\n",
      "  (0): ConvTranspose2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_acts = [\"none\", \"linear\"]\n",
    "for act in linear_acts:\n",
    "    upconv_linear = UpsampleConv2d(in_channels, out_channels, norm_type=\"none\", act=act)\n",
    "    print(upconv_linear)\n",
    "    assert len(upconv_linear) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UnsqueezeLatent(nn.Module):\n",
    "    \"\"\"將 latent vector unsqueeze\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x[..., None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "latent_dim = 100\n",
    "\n",
    "x = torch.randn(batch_size, latent_dim)\n",
    "out = UnsqueezeLatent()(x)\n",
    "assert out.shape == (batch_size, latent_dim, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SqueezeLogit(nn.Module):\n",
    "    \"\"\"Squeeze Discriminator logit\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x.squeeze(-1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dim = 50\n",
    "\n",
    "x = torch.randn(batch_size, dim, 1, 1)\n",
    "out = SqueezeLogit()(x)\n",
    "assert out.shape == (batch_size, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DownsampleConv2d(nn.Sequential):\n",
    "    \"\"\"基本下採樣： Conv2d -> Norm -> Activation\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 stride=2,\n",
    "                 padding=1,\n",
    "                 norm_type=\"batch\",\n",
    "                 act=\"leaky_relu\",\n",
    "                 bias=True):\n",
    "        \n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)]\n",
    "        \n",
    "        if norm_type != \"none\":\n",
    "            layers.append(get_norm2d(norm_type)(out_channels))\n",
    "            \n",
    "        layers.append(get_activation(act))\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "in_channels = 3\n",
    "out_channels = 64\n",
    "h = w = 32\n",
    "\n",
    "x = torch.randn(batch_size, in_channels, h, w)\n",
    "downconv = DownsampleConv2d(in_channels, out_channels)\n",
    "out = downconv(x)\n",
    "assert out.shape == (batch_size, out_channels, h / 2, w / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvGenerator(nn.Sequential):\n",
    "    \"\"\"將特定維度的潛在向量上採樣到指定圖片大小的生成器\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 latent_dim=LATENT_DIM,\n",
    "                 out_dim=DIM,\n",
    "                 out_channels=CHANNELS,\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 max_channels=None,\n",
    "                 norm_type=NORM_TYPE,\n",
    "                 act=\"relu\",\n",
    "                 dim_channel_multiplier=DIM_CHANNEL_MULTIPLIER):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dim_channel_multiplier = dim_channel_multiplier\n",
    "        self.norm_type = norm_type\n",
    "        self.act = act\n",
    "        self.max_channels = max_channels if max_channels else self.out_dim * self.dim_channel_multiplier\n",
    "        \n",
    "        # decide appropriate number of upsampling process based on expected output image shape\n",
    "        self.n_upsamples = get_n_samplings(self.out_dim)\n",
    "        \n",
    "        # projected to spatial extent convolutional repr. with feature maps\n",
    "        # x.shape == (batch_size, latent_dim)\n",
    "        layers = [\n",
    "            UnsqueezeLatent(),\n",
    "            UpsampleConv2d(in_channels=self.latent_dim,\n",
    "                           out_channels=self.max_channels,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           stride=1,  # no need to stride in first layer\n",
    "                           padding=0,  # no padding in first layer\n",
    "                           norm_type=self.norm_type,\n",
    "                           act=self.act)]\n",
    "        \n",
    "        # upsamples\n",
    "        # x.shape == (batch_size, max_channels, kernel_size, kernel_size)\n",
    "        chs = [self.max_channels // (2 ** i) for i in range(self.n_upsamples)]\n",
    "        chs.append(self.out_channels)\n",
    "        \n",
    "        layers.extend([\n",
    "            UpsampleConv2d(in_channels=in_ch,\n",
    "                           out_channels=out_ch,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           stride=2,\n",
    "                           norm_type=self.norm_type if i != self.n_upsamples else \"none\",\n",
    "                           act=self.act if i != self.n_upsamples else \"tanh\",\n",
    "                           bias=False if i != self.n_upsamples else True)\n",
    "         for i, (in_ch, out_ch) in enumerate(pairwise(chs), 1)])\n",
    "        # out.shape == (batch_size, out_channels, out_dim, out_dim)\n",
    "        \n",
    "        # final act: tanh\n",
    "        # using a bounded activation allowed the model to learn more quickly to \n",
    "        # saturate and cover the color space of the training distribution. \n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvGenerator(\n",
       "  (0): UnsqueezeLatent()\n",
       "  (1): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (2): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (3): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (4): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "for latent_dim, out_dim, out_ch in zip([128, 50, 100], [128, 64, 32], [3, 3, 1]):\n",
    "    x = torch.randn(batch_size, latent_dim)\n",
    "    g = ConvGenerator(latent_dim=latent_dim, out_dim=out_dim, out_channels=out_ch)\n",
    "    out = g(x)\n",
    "    assert out.shape == (batch_size, out_ch, out_dim, out_dim)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvDiscriminator(nn.Sequential):\n",
    "    \"\"\"將特定大小圖片下採樣的辨識器\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels=CHANNELS, \n",
    "                 in_dim=DIM, \n",
    "                 norm_type=NORM_TYPE,\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 max_channels=None,\n",
    "                 dim_channel_multiplier=DIM_CHANNEL_MULTIPLIER):\n",
    "        self.in_channels = in_channels\n",
    "        self.in_dim = in_dim\n",
    "        self.norm_type = norm_type\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_downsamples = get_n_samplings(self.in_dim)\n",
    "        self.dim_channel_multiplier = dim_channel_multiplier\n",
    "        self.max_channels = max_channels if max_channels else self.in_dim * self.dim_channel_multiplier\n",
    "        \n",
    "        # downsample\n",
    "        chs = [self.in_channels]\n",
    "        chs += sorted([self.max_channels // (2 ** i) for i in range(self.n_downsamples)])\n",
    "        \n",
    "        # x.shape == (batch_size, in_channels, in_dim, in_dim)\n",
    "        layers = [\n",
    "            DownsampleConv2d(in_ch, \n",
    "                             out_ch, \n",
    "                             self.kernel_size, \n",
    "                             stride=2, \n",
    "                             norm_type=self.norm_type if i != 1 else \"none\",\n",
    "                             bias=False if i != 1 else True)\n",
    "            for i, (in_ch, out_ch) in enumerate(pairwise(chs), 1)]\n",
    "        \n",
    "        # compute logits\n",
    "        # x.shape == (batch_size, max_channels, kernel_size, kernel_size)\n",
    "        layers.extend([\n",
    "            nn.Conv2d(chs[-1], 1, kernel_size=self.kernel_size),\n",
    "            SqueezeLogit()\n",
    "        ])\n",
    "        # out.shape == (batch_size, 1)\n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvDiscriminator(\n",
       "  (0): DownsampleConv2d(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (1): DownsampleConv2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (2): DownsampleConv2d(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (3): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (4): SqueezeLogit()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "for in_ch, in_dim in zip([3, 3, 1], [128, 64, 32]):\n",
    "    x = torch.randn(batch_size, in_ch, in_dim, in_dim)\n",
    "    d = ConvDiscriminator(in_ch, in_dim)\n",
    "    out = d(x)\n",
    "    assert out.shape == (batch_size, 1)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_generater(_type):\n",
    "    if _type == \"conv\":\n",
    "        return ConvGenerator\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def get_discriminator(_type):\n",
    "    if _type == \"conv\":\n",
    "        return ConvDiscriminator\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GAN(pl.LightningModule):\n",
    "    \"\"\"對抗生成網路\"\"\"\n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super(GAN, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # adversarial losses\n",
    "        self.g_loss_fn, self.d_loss_fn = \\\n",
    "            get_adversarial_losses_fns(self.hparams.adversarial_loss_type)\n",
    "        \n",
    "        # infer image size by dataset\n",
    "        \n",
    "        \n",
    "        \n",
    "        # initialize networks\n",
    "        g = get_generater(self.hparams.generator_type)\n",
    "        self.generator = g(latent_dim=self.hparams.latent_dim, \n",
    "                           out_dim=self.hparams.dim, \n",
    "                           out_channels=self.hparams.channels,\n",
    "                           kernel_size=self.hparams.kernel_size,\n",
    "                           norm_type=self.hparams.norm_type)\n",
    "        \n",
    "        d = get_discriminator(self.hparams.discriminator_type)\n",
    "        self.discriminator = d(in_channels=self.hparams.channels,\n",
    "                               in_dim=self.hparams.dim,\n",
    "                               norm_type=self.hparams.norm_type,\n",
    "                               kernel_size=self.hparams.kernel_size)\n",
    "        \n",
    "        # cache for generated images\n",
    "        self.generated_images = None\n",
    "        self.last_real_images = None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.train_dataset = get_dataset(dataset_name=self.hparams.dataset,\n",
    "                                         split=\"train\",\n",
    "                                         size=(self.hparams.dim, self.hparams.dim), \n",
    "                                         return_label=False)\n",
    "        \n",
    "        self.valid_dataset = get_dataset(dataset_name=self.hparams.dataset,\n",
    "                                         split=\"valid\",\n",
    "                                         size=(self.hparams.dim, self.hparams.dim), \n",
    "                                         return_label=False)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return get_data_loader(self.train_dataset, batch_size=self.hparams.batch_size)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        self.g_optim = torch.optim.Adam(self.generator.parameters(), \n",
    "                                        lr=self.hparams.lr, \n",
    "                                        betas=(self.hparams.beta1, self.hparams.beta2))\n",
    "        self.d_optim = torch.optim.Adam(self.discriminator.parameters(), \n",
    "                                        lr=self.hparams.lr, \n",
    "                                        betas=(self.hparams.beta1, self.hparams.beta2))\n",
    "        return [self.d_optim, self.g_optim], []\n",
    "    \n",
    "    def get_latent_vectors(self, n, on_gpu=True):\n",
    "        z = torch.randn(n, self.hparams.latent_dim)\n",
    "        if on_gpu:\n",
    "            z = z.cuda(self.last_real_images.device.index)\n",
    "        return z\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        self.last_real_images = real_images = batch\n",
    "        z = self.get_latent_vectors(n=self.hparams.batch_size, on_gpu=self.on_gpu)\n",
    "        \n",
    "        # discriminator\n",
    "        if optimizer_idx == 0:\n",
    "            fake_images = self.generator(z).detach()\n",
    "            real_logits = self.discriminator(real_images)\n",
    "            fake_logits = self.discriminator(fake_images)\n",
    "            \n",
    "            d_real_loss, d_fake_loss = self.d_loss_fn(real_logits, fake_logits, \n",
    "                                                      on_gpu=self.on_gpu)\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            \n",
    "            # TODO: gradient penality\n",
    "            \n",
    "            tqdm_dict = {'d_loss': d_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': d_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "            \n",
    "        # generator\n",
    "        if optimizer_idx == 1:\n",
    "            fake_images = self.generateed_images = self.generator(z)\n",
    "            fake_logits = self.discriminator(fake_images)\n",
    "            g_loss = self.g_loss_fn(fake_logits)\n",
    "            \n",
    "            tqdm_dict = {'g_loss': g_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': g_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "    \n",
    "#     def on_train_start(self):\n",
    "#         # https://github.com/PyTorchLightning/pytorch-lightning/blob/af621f8590b2f2ba046b508da2619cfd4995d876/pytorch_lightning/core/hooks.py#L45-L49\n",
    "#         # https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_hparams\n",
    "#         hparam_dict = {}\n",
    "#         metric_dict = {}\n",
    "#         self.logger.experiment.add_hparams({'lr': 0.1*i, 'bsize': i},{})\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        z = self.get_latent_vectors(n=64, on_gpu=self.on_gpu)\n",
    "        sample_images = self.generator(z)\n",
    "        grid = torchvision.utils.make_grid(sample_images)\n",
    "        self.logger.experiment.add_image('sample_images', grid, self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "notebook2script()\n",
    "clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_practical_ai",
   "language": "python",
   "name": "conda_practical_ai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
