---

title: vision.gan.gp

keywords: fastai
sidebar: home_sidebar

summary: "實作 Gradient Penality。"
description: "實作 Gradient Penality。"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/vision.gan.gp.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>cannot use batch normalization with gradient penalty
<code>d_norm = args.gradient_penalty_d_norm</code></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># lambda_gp = 10</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def compute_gradient_penalty(D, real_samples, fake_samples):</span>
<span class="c1">#     &quot;&quot;&quot;Calculates the gradient penalty loss for WGAN GP&quot;&quot;&quot;</span>
<span class="c1">#     # Random weight term for interpolation between real and fake samples</span>
<span class="c1">#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))</span>
<span class="c1">#     # Get random interpolation between real and fake samples</span>
<span class="c1">#     interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)</span>
<span class="c1">#     d_interpolates = D(interpolates)</span>
<span class="c1">#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)</span>
<span class="c1">#     # Get gradient w.r.t. interpolates</span>
<span class="c1">#     gradients = autograd.grad(</span>
<span class="c1">#         outputs=d_interpolates,</span>
<span class="c1">#         inputs=interpolates,</span>
<span class="c1">#         grad_outputs=fake,</span>
<span class="c1">#         create_graph=True,</span>
<span class="c1">#         retain_graph=True,</span>
<span class="c1">#         only_inputs=True,</span>
<span class="c1">#     )[0]</span>
<span class="c1">#     gradients = gradients.view(gradients.size(0), -1)</span>
<span class="c1">#     gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()</span>
<span class="c1">#     return gradient_penalty</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)</span>

<span class="c1"># loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))</span>
<span class="c1"># d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

