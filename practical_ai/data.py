# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/data.ipynb (unless otherwise specified).

__all__ = ['logger', 'get_data_root', 'ImageOnlyDataset', 'Stickers', 'get_dataset', 'get_data_loader']


# Cell
import os
import torch
import logging
import warnings
import multiprocessing
from PIL import Image
from dotenv import load_dotenv
from easydict import EasyDict as edict
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset

_ = load_dotenv()

logger = logging.getLogger()
logger.setLevel("INFO")


# Cell
def get_data_root(data_root=None):
    data_root = data_root if data_root else os.getenv("DATA_ROOT", ".")
    if not os.path.exists(data_root):
        os.makedirs(data_root)
    return data_root


# Cell
class ImageOnlyDataset(Dataset):
    """常用於生成任務，只回傳圖片而不回傳標籤的 Dataset"""

    def __init__(self, img_label_dataset, img_idx=0):
        self.orig_dataset = img_label_dataset
        self.img_idx = img_idx

    def __len__(self):
        return len(self.orig_dataset)

    def __getitem__(self, idx):
        return self.orig_dataset[idx][self.img_idx]


# Cell
class Stickers(Dataset):
    def __init__(self, sticker_name=None, size=None):
        # files
        self.root_dir = os.path.join(get_data_root(), "stickers")
        self.sticker_name = sticker_name if sticker_name else "kanahei"
        self.size = size if size else (128, 128)
        self.sticker_dir = os.path.join(self.root_dir, self.sticker_name)
        self.image_paths = None

        for dirpath, _, filenames in os.walk(self.root_dir):
            if dirpath == self.sticker_dir:
                self.image_paths = [os.path.join(dirpath, f) for f in filenames]
                break

        if not self.image_paths:
            logging.warning(f"{self.sticker_dir} does not exist.")

        # stats
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]

        transform = [
            transforms.ColorJitter(.1, .1, .1, .1),
            transforms.RandomHorizontalFlip(),
#             transforms.Resize((224, 256)),
            transforms.Resize(self.size),
#             transforms.CenterCrop(self.size),
            transforms.ToTensor(),
#             transforms.Normalize(mean=self.mean, std=self.std),
            transforms.Normalize(mean=[0.5], std=[0.5])
        ]
        self.transform = transforms.Compose(transform)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', UserWarning)
            image = Image.open(self.image_paths[idx]).convert("RGB")
        image = self.transform(image)
        return image


# Cell
def get_dataset(dataset, split="full", size=None, transform=None, return_label=True,
                **kwargs):

    dataset = dataset.lower()
    if dataset == "mnist":
        size = size if size else (28, 28)
        logging.info(f"MNIST will be resized to {size}.")

        transform = transforms.Compose([
            transforms.Resize(size=size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5], std=[0.5])
        ]) if not transform else transform

        root = get_data_root()
        ds_params = dict(root=root, transform=transform, download=True)
        if os.path.exists(os.path.join(root, "MNIST")):
            ds_params['download'] = False

        if split == "train":
            ds_params['train'] = True
        elif split == "test":
            ds_params['train'] = False
        dataset = datasets.MNIST(**ds_params)

        if not return_label:
            dataset = ImageOnlyDataset(dataset)


    elif dataset == "stickers":
        sticker_name = kwargs.get("sticker_name", "kanahei")
        size = size if size else (128, 128)
        dataset = Stickers(sticker_name, size=size)
    else:
        raise NotImplementedError

    setattr(dataset, "input_shape", (1, *size))
    return dataset


# Cell
def get_data_loader(dataset, batch_size, shuffle=True, collate_fn=None, drop_last=True, **kwargs):
    use_cuda = torch.cuda.is_available()
    num_workers = multiprocessing.cpu_count() if use_cuda else 1

    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,
                             num_workers=num_workers, collate_fn=collate_fn,
                             drop_last=drop_last, pin_memory=use_cuda)
    return data_loader