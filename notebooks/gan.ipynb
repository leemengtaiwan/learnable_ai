{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from IPython.display import clear_output\n",
    "from nbdev.export import notebook2script\n",
    "from dotenv import load_dotenv\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import logging\n",
    "import functools\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from more_itertools import pairwise\n",
    "from collections import OrderedDict\n",
    "from practical_ai.layers import Identity\n",
    "from practical_ai.data import get_dataset, get_data_loader\n",
    "from practical_ai.gan.loss import get_adversarial_losses_fns\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gan\n",
    "\n",
    "> 對抗生成網路（Generative Adversarial Network）的相關模組。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "DIM_CHANNEL_MULTIPLIER = 8\n",
    "KERNEL_SIZE = 4\n",
    "LATENT_DIM = 128\n",
    "DIM = 32\n",
    "CHANNELS = 1\n",
    "NORM = \"batch\"\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_n_samplings(dim):\n",
    "    return int(torch.log2(torch.tensor(dim, dtype=torch.float32)).item()) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 32\n",
    "assert get_n_samplings(32) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_norm2d(name):\n",
    "    if name == \"identity\":\n",
    "        return Identity\n",
    "    elif name == \"batch\":\n",
    "        return nn.BatchNorm2d\n",
    "    elif name == \"instance\":\n",
    "        return functools.partial(nn.InstanceNorm2d, affine=True)\n",
    "    elif name == \"layer\":\n",
    "        return lambda num_features: nn.GroupNorm(1, num_features)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "practical_ai.layers.Identity"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = get_norm2d(\"identity\")\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_activation(name):\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif name == \"leaky_relu\":\n",
    "        return nn.LeakyReLU(0.2)\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = get_activation(\"relu\")\n",
    "act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UpsampleConv2d(nn.Sequential):\n",
    "    \"\"\"基本上採樣： ConvTransponse2d -> Norm -> Activation\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 stride=2,\n",
    "                 padding=1,\n",
    "                 norm=\"batch\",\n",
    "                 act=\"relu\",\n",
    "                 bias=True):\n",
    "        \n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, \n",
    "                               out_channels, \n",
    "                               kernel_size, \n",
    "                               stride, \n",
    "                               padding,\n",
    "                               bias=bias)]\n",
    "        \n",
    "        if norm != \"none\":\n",
    "            layers.append(get_norm2d(norm)(out_channels))\n",
    "        \n",
    "        if act not in [\"none\", \"linear\"]:\n",
    "            layers.append(get_activation(act))\n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般使用： Transposed Conv -> Norm -> Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpsampleConv2d(\n",
       "  (0): ConvTranspose2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 3\n",
    "out_channels = 128\n",
    "h = w = 32\n",
    "batch_size = 8\n",
    "\n",
    "x = torch.randn(batch_size, in_channels, h, w)\n",
    "upconv = UpsampleConv2d(in_channels, out_channels)\n",
    "out = upconv(x)\n",
    "\n",
    "assert len(upconv) == 3\n",
    "assert out.shape == (batch_size, out_channels, h * 2, w * 2)\n",
    "upconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "單純上採樣，不使用 Norm 以及非線性 activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UpsampleConv2d(\n",
      "  (0): ConvTranspose2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      ")\n",
      "UpsampleConv2d(\n",
      "  (0): ConvTranspose2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_acts = [\"none\", \"linear\"]\n",
    "for act in linear_acts:\n",
    "    upconv_linear = UpsampleConv2d(in_channels, out_channels, norm=\"none\", act=act)\n",
    "    print(upconv_linear)\n",
    "    assert len(upconv_linear) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UnsqueezeLatent(nn.Module):\n",
    "    \"\"\"將 latent vector unsqueeze\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x[..., None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "latent_dim = 100\n",
    "\n",
    "x = torch.randn(batch_size, latent_dim)\n",
    "out = UnsqueezeLatent()(x)\n",
    "assert out.shape == (batch_size, latent_dim, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SqueezeLogit(nn.Module):\n",
    "    \"\"\"Squeeze Discriminator logit\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x.squeeze(-1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dim = 50\n",
    "\n",
    "x = torch.randn(batch_size, dim, 1, 1)\n",
    "out = SqueezeLogit()(x)\n",
    "assert out.shape == (batch_size, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DownsampleConv2d(nn.Sequential):\n",
    "    \"\"\"基本下採樣： Conv2d -> Norm -> Activation\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 stride=2,\n",
    "                 padding=1,\n",
    "                 norm=\"batch\",\n",
    "                 act=\"leaky_relu\",\n",
    "                 bias=True):\n",
    "        \n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)]\n",
    "        \n",
    "        if norm != \"none\":\n",
    "            layers.append(get_norm2d(norm)(out_channels))\n",
    "            \n",
    "        layers.append(get_activation(act))\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "in_channels = 3\n",
    "out_channels = 64\n",
    "h = w = 32\n",
    "\n",
    "x = torch.randn(batch_size, in_channels, h, w)\n",
    "downconv = DownsampleConv2d(in_channels, out_channels)\n",
    "out = downconv(x)\n",
    "assert out.shape == (batch_size, out_channels, h / 2, w / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvGenerator(nn.Sequential):\n",
    "    \"\"\"將特定維度的潛在向量上採樣到指定圖片大小的生成器\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 latent_dim=LATENT_DIM,\n",
    "                 out_dim=DIM,\n",
    "                 out_channels=CHANNELS,\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 max_channels=None,\n",
    "                 norm=NORM,\n",
    "                 act=\"relu\",\n",
    "                 dim_channel_multiplier=DIM_CHANNEL_MULTIPLIER):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dim_channel_multiplier = dim_channel_multiplier\n",
    "        self.norm = norm\n",
    "        self.act = act\n",
    "        self.max_channels = max_channels if max_channels else self.out_dim * self.dim_channel_multiplier\n",
    "        \n",
    "        # decide appropriate number of upsampling process based on expected output image shape\n",
    "        self.n_upsamples = get_n_samplings(self.out_dim)\n",
    "        \n",
    "        # projected to spatial extent convolutional repr. with feature maps\n",
    "        # x.shape == (batch_size, latent_dim)\n",
    "        layers = [\n",
    "            UnsqueezeLatent(),\n",
    "            UpsampleConv2d(in_channels=self.latent_dim,\n",
    "                           out_channels=self.max_channels,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           stride=1,  # no need to stride in first layer\n",
    "                           padding=0,  # no padding in first layer\n",
    "                           norm=self.norm,\n",
    "                           act=self.act)]\n",
    "        \n",
    "        # upsamples\n",
    "        # x.shape == (batch_size, max_channels, kernel_size, kernel_size)\n",
    "        chs = [self.max_channels // (2 ** i) for i in range(self.n_upsamples)]\n",
    "        chs.append(self.out_channels)\n",
    "        \n",
    "        layers.extend([\n",
    "            UpsampleConv2d(in_channels=in_ch,\n",
    "                           out_channels=out_ch,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           stride=2,\n",
    "                           norm=self.norm if i != self.n_upsamples else \"none\",\n",
    "                           act=self.act if i != self.n_upsamples else \"tanh\",\n",
    "                           bias=False if i != self.n_upsamples else True)\n",
    "         for i, (in_ch, out_ch) in enumerate(pairwise(chs), 1)])\n",
    "        # out.shape == (batch_size, out_channels, out_dim, out_dim)\n",
    "        \n",
    "        # final act: tanh\n",
    "        # using a bounded activation allowed the model to learn more quickly to \n",
    "        # saturate and cover the color space of the training distribution. \n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvGenerator(\n",
       "  (0): UnsqueezeLatent()\n",
       "  (1): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (2): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (3): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (4): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "for latent_dim, out_dim, out_ch in zip([128, 50, 100], [128, 64, 32], [3, 3, 1]):\n",
    "    x = torch.randn(batch_size, latent_dim)\n",
    "    g = ConvGenerator(latent_dim=latent_dim, out_dim=out_dim, out_channels=out_ch)\n",
    "    out = g(x)\n",
    "    assert out.shape == (batch_size, out_ch, out_dim, out_dim)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDiscriminator(nn.Sequential):\n",
    "    \"\"\"將特定大小圖片下採樣的辨識器\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels=CHANNELS, \n",
    "                 in_dim=DIM, \n",
    "                 norm=NORM,\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 max_channels=None,\n",
    "                 dim_channel_multiplier=DIM_CHANNEL_MULTIPLIER):\n",
    "        self.in_channels = in_channels\n",
    "        self.in_dim = in_dim\n",
    "        self.norm = norm\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_downsamples = get_n_samplings(self.in_dim)\n",
    "        self.dim_channel_multiplier = dim_channel_multiplier\n",
    "        self.max_channels = max_channels if max_channels else self.in_dim * self.dim_channel_multiplier\n",
    "        \n",
    "        # downsample\n",
    "        chs = [self.in_channels]\n",
    "        chs += sorted([self.max_channels // (2 ** i) for i in range(self.n_downsamples)])\n",
    "        \n",
    "        # x.shape == (batch_size, in_channels, in_dim, in_dim)\n",
    "        layers = [\n",
    "            DownsampleConv2d(in_ch, \n",
    "                             out_ch, \n",
    "                             self.kernel_size, \n",
    "                             stride=2, \n",
    "                             norm=self.norm if i != 1 else \"none\",\n",
    "                             bias=False if i != 1 else True)\n",
    "            for i, (in_ch, out_ch) in enumerate(pairwise(chs), 1)]\n",
    "        \n",
    "        # compute logits\n",
    "        # x.shape == (batch_size, max_channels, kernel_size, kernel_size)\n",
    "        layers.extend([\n",
    "            nn.Conv2d(chs[-1], 1, kernel_size=self.kernel_size),\n",
    "            SqueezeLogit()\n",
    "        ])\n",
    "        # out.shape == (batch_size, 1)\n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvDiscriminator(\n",
       "  (0): DownsampleConv2d(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (1): DownsampleConv2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (2): DownsampleConv2d(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (3): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (4): SqueezeLogit()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "for in_ch, in_dim in zip([3, 3, 1], [128, 64, 32]):\n",
    "    x = torch.randn(batch_size, in_ch, in_dim, in_dim)\n",
    "    d = ConvDiscriminator(in_ch, in_dim)\n",
    "    out = d(x)\n",
    "    assert out.shape == (batch_size, 1)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_generater(_type):\n",
    "    if _type == \"conv\":\n",
    "        return ConvGenerator\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def get_discriminator(_type):\n",
    "    if _type == \"conv\":\n",
    "        return ConvDiscriminator\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GAN(pl.LightningModule):\n",
    "    \"\"\"基本對抗生成網路\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 generator_type=\"conv\", \n",
    "                 discriminator_type=\"conv\",\n",
    "                 dataset_name=\"mnist\",\n",
    "                 adversarial_loss_type=\"gan\",\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 lr=2e-4,\n",
    "                 beta1=0.5,\n",
    "                 beta2=0.999,\n",
    "                 latent_dim=LATENT_DIM,\n",
    "                 image_shape=(CHANNELS, DIM, DIM),\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 norm=NORM,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.generator_type = generator_type\n",
    "        self.discriminator_type = discriminator_type\n",
    "        self.dataset_name = dataset_name\n",
    "        self.adversarial_loss_type = adversarial_loss_type\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.image_shape = image_shape\n",
    "        self.kernel_size = kernel_size\n",
    "        self.norm = norm\n",
    "        \n",
    "        # adversarial losses\n",
    "        self.g_loss_fn, self.d_loss_fn = \\\n",
    "            get_adversarial_losses_fns(self.adversarial_loss_type)\n",
    "        \n",
    "        \n",
    "        assert self.image_shape[-1] == self.image_shape[-2]\n",
    "        self.channels, self.dim = self.image_shape[0], self.image_shape[-1]\n",
    "        \n",
    "        # initialize networks\n",
    "        g = get_generater(self.generator_type)\n",
    "        self.generator = g(latent_dim=self.latent_dim, \n",
    "                           out_dim=self.dim, \n",
    "                           out_channels=self.channels,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           norm=self.norm)\n",
    "        \n",
    "        d = get_discriminator(self.discriminator_type)\n",
    "        self.discriminator = d(in_channels=self.channels,\n",
    "                               in_dim=self.dim,\n",
    "                               norm=self.norm,\n",
    "                               kernel_size=self.kernel_size)\n",
    "        \n",
    "        # cache for generated images\n",
    "        self.generated_images = None\n",
    "        self.last_real_images = None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.train_dataset = get_dataset(dataset=self.dataset_name,\n",
    "                                         split=\"train\",\n",
    "                                         size=(self.dim, self.dim), \n",
    "                                         return_label=False)\n",
    "        \n",
    "        self.valid_dataset = get_dataset(dataset=self.dataset_name,\n",
    "                                         split=\"valid\",\n",
    "                                         size=(self.dim, self.dim), \n",
    "                                         return_label=False)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return get_data_loader(self.train_dataset, batch_size=self.batch_size)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        self.g_optim = torch.optim.Adam(self.generator.parameters(), \n",
    "                                        lr=self.lr, \n",
    "                                        betas=(self.beta1, self.beta2))\n",
    "        self.d_optim = torch.optim.Adam(self.discriminator.parameters(), \n",
    "                                        lr=self.lr, \n",
    "                                        betas=(self.beta1, self.beta2))\n",
    "        return [self.d_optim, self.g_optim], []\n",
    "    \n",
    "    def get_latent_vectors(self, n=8, on_gpu=True):\n",
    "        z = torch.randn(n, self.latent_dim)\n",
    "        if on_gpu:\n",
    "            z = z.cuda(self.last_real_images.device.index)\n",
    "        return z\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        self.last_real_images = real_images = batch\n",
    "        z = self.get_latent_vectors(on_gpu=self.on_gpu)\n",
    "        \n",
    "        # discriminator\n",
    "        if optimizer_idx == 0:\n",
    "            fake_images = self.generator(z).detach()\n",
    "            real_logits = self.discriminator(real_images)\n",
    "            fake_logits = self.discriminator(fake_images)\n",
    "            \n",
    "            d_real_loss, d_fake_loss = self.d_loss_fn(real_logits, fake_logits, \n",
    "                                                      on_gpu=self.on_gpu)\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            \n",
    "            # TODO: gradient penality\n",
    "            \n",
    "            tqdm_dict = {'d_loss': d_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': d_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "            \n",
    "        # generator\n",
    "        if optimizer_idx == 1:\n",
    "            fake_images = self.generateed_images = self.generator(z)\n",
    "            fake_logits = self.discriminator(fake_images)\n",
    "            g_loss = self.g_loss_fn(fake_logits)\n",
    "            \n",
    "            tqdm_dict = {'g_loss': g_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': g_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        z = self.get_latent_vectors(on_gpu=self.on_gpu)\n",
    "        sample_images = self.generator(z)\n",
    "        grid = torchvision.utils.make_grid(sample_images)\n",
    "        self.logger.experiment.add_image('sample_images', grid, self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GPU available: True, used: True\n",
      "INFO:root:VISIBLE GPUS: 0\n",
      "INFO:root:MNIST will be resized to (32, 32).\n",
      "INFO:root:MNIST will be resized to (32, 32).\n",
      "INFO:root:\n",
      "   | Name              | Type              | Params\n",
      "----------------------------------------------------\n",
      "0  | generator         | ConvGenerator     | 1 M   \n",
      "1  | generator.0       | UnsqueezeLatent   | 0     \n",
      "2  | generator.1       | UpsampleConv2d    | 525 K \n",
      "3  | generator.1.0     | ConvTranspose2d   | 524 K \n",
      "4  | generator.1.1     | BatchNorm2d       | 512   \n",
      "5  | generator.1.2     | ReLU              | 0     \n",
      "6  | generator.2       | UpsampleConv2d    | 524 K \n",
      "7  | generator.2.0     | ConvTranspose2d   | 524 K \n",
      "8  | generator.2.1     | BatchNorm2d       | 256   \n",
      "9  | generator.2.2     | ReLU              | 0     \n",
      "10 | generator.3       | UpsampleConv2d    | 131 K \n",
      "11 | generator.3.0     | ConvTranspose2d   | 131 K \n",
      "12 | generator.3.1     | BatchNorm2d       | 128   \n",
      "13 | generator.3.2     | ReLU              | 0     \n",
      "14 | generator.4       | UpsampleConv2d    | 1 K   \n",
      "15 | generator.4.0     | ConvTranspose2d   | 1 K   \n",
      "16 | generator.4.1     | Tanh              | 0     \n",
      "17 | discriminator     | ConvDiscriminator | 661 K \n",
      "18 | discriminator.0   | DownsampleConv2d  | 1 K   \n",
      "19 | discriminator.0.0 | Conv2d            | 1 K   \n",
      "20 | discriminator.0.1 | LeakyReLU         | 0     \n",
      "21 | discriminator.1   | DownsampleConv2d  | 131 K \n",
      "22 | discriminator.1.0 | Conv2d            | 131 K \n",
      "23 | discriminator.1.1 | BatchNorm2d       | 256   \n",
      "24 | discriminator.1.2 | LeakyReLU         | 0     \n",
      "25 | discriminator.2   | DownsampleConv2d  | 524 K \n",
      "26 | discriminator.2.0 | Conv2d            | 524 K \n",
      "27 | discriminator.2.1 | BatchNorm2d       | 512   \n",
      "28 | discriminator.2.2 | LeakyReLU         | 0     \n",
      "29 | discriminator.3   | Conv2d            | 4 K   \n",
      "30 | discriminator.4   | SqueezeLogit      | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d0e8cf53604552aa20800a3abcb3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#server\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "args = {\n",
    "#     'batch_size': 32,\n",
    "#     'lr': 0.0002,\n",
    "#     'b1': 0.5,\n",
    "#     'b2': 0.999,\n",
    "#     'latent_dim': 100\n",
    "}\n",
    "hparams = Namespace(**args)\n",
    "\n",
    "gan = GAN()\n",
    "\n",
    "# most basic trainer, uses good defaults (1 gpu)\n",
    "trainer = pl.Trainer(gpus=1)    \n",
    "trainer.fit(gan)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "notebook2script()\n",
    "clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_practical_ai",
   "language": "python",
   "name": "conda_practical_ai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
