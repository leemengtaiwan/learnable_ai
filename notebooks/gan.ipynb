{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from IPython.display import clear_output\n",
    "from nbdev.export import notebook2script\n",
    "from dotenv import load_dotenv\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import logging\n",
    "import functools\n",
    "from torch import nn\n",
    "from more_itertools import pairwise\n",
    "from practical_ai.layers import Identity\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gan\n",
    "\n",
    "> 對抗生成網路（Generative Adversarial Network）的相關模組。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "DIM_CHANNEL_MULTIPLIER = 8\n",
    "KERNEL_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_n_samplings(dim):\n",
    "    return int(torch.log2(torch.tensor(dim, dtype=torch.float32)).item()) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 32\n",
    "assert get_n_samplings(32) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_norm2d(name):\n",
    "    if name == \"identity\":\n",
    "        return Identity\n",
    "    elif name == \"batch\":\n",
    "        return nn.BatchNorm2d\n",
    "    elif name == \"instance\":\n",
    "        return functools.partial(nn.InstanceNorm2d, affine=True)\n",
    "    elif name == \"layer\":\n",
    "        return lambda num_features: nn.GroupNorm(1, num_features)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "practical_ai.layers.Identity"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = get_norm2d(\"identity\")\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_activation(name):\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif name == \"leaky_relu\":\n",
    "        return nn.LeakyReLU(0.2)\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = get_activation(\"relu\")\n",
    "act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UpsampleConv2d(nn.Sequential):\n",
    "    \"\"\"ConvTransponse2d -> Normalization -> Activation\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 stride=2,\n",
    "                 padding=1,\n",
    "                 norm=\"batch\",\n",
    "                 act=\"relu\",\n",
    "                 bias=True):\n",
    "        \n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, \n",
    "                               out_channels, \n",
    "                               kernel_size, \n",
    "                               stride, \n",
    "                               padding,\n",
    "                               bias=bias)]\n",
    "        \n",
    "        if norm != \"none\":\n",
    "            layers.append(get_norm2d(norm)(out_channels))\n",
    "        \n",
    "        if act not in [\"none\", \"linear\"]:\n",
    "            layers.append(get_activation(act))\n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般使用： Transposed Conv -> Norm -> Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpsampleConv2d(\n",
       "  (0): ConvTranspose2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 3\n",
    "out_channels = 128\n",
    "h = w = 32\n",
    "batch_size = 8\n",
    "\n",
    "x = torch.randn(batch_size, in_channels, h, w)\n",
    "upconv = UpsampleConv2d(in_channels, out_channels)\n",
    "out = upconv(x)\n",
    "\n",
    "assert len(upconv) == 3\n",
    "assert out.shape == (batch_size, out_channels, h * 2, w * 2)\n",
    "upconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "單純上採樣，不使用 Norm 以及非線性 activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UpsampleConv2d(\n",
      "  (0): ConvTranspose2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      ")\n",
      "UpsampleConv2d(\n",
      "  (0): ConvTranspose2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_acts = [\"none\", \"linear\"]\n",
    "for act in linear_acts:\n",
    "    upconv_linear = UpsampleConv2d(in_channels, out_channels, norm=\"none\", act=act)\n",
    "    print(upconv_linear)\n",
    "    assert len(upconv_linear) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UnsqueezeLatent(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x[..., None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "latent_dim = 100\n",
    "\n",
    "x = torch.randn(batch_size, latent_dim)\n",
    "out = UnsqueezeLatent()(x)\n",
    "assert out.shape == (batch_size, latent_dim, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SqueezeLogit(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.squeeze(-1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dim = 50\n",
    "\n",
    "x = torch.randn(batch_size, dim, 1, 1)\n",
    "out = SqueezeLogit()(x)\n",
    "assert out.shape == (batch_size, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DownsampleConv2d(nn.Sequential):\n",
    "    \"\"\"Conv2D -> Normalization -> Activation\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 stride=2,\n",
    "                 padding=1,\n",
    "                 norm=\"batch\",\n",
    "                 act=\"leaky_relu\",\n",
    "                 bias=True):\n",
    "        \n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)]\n",
    "        \n",
    "        if norm != \"none\":\n",
    "            layers.append(get_norm2d(norm)(out_channels))\n",
    "            \n",
    "        layers.append(get_activation(act))\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本下採樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "in_channels = 3\n",
    "out_channels = 64\n",
    "h = w = 32\n",
    "\n",
    "x = torch.randn(batch_size, in_channels, h, w)\n",
    "downconv = DownsampleConv2d(in_channels, out_channels)\n",
    "out = downconv(x)\n",
    "assert out.shape == (batch_size, out_channels, h / 2, w / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvGenerator(nn.Sequential):\n",
    "    \"\"\"將輸入維度的隨機向量上採樣到指定大小圖片的生成器\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 latent_dim=128,\n",
    "                 out_dim=32,\n",
    "                 out_channels=1,\n",
    "                 kernel_size=4,\n",
    "                 max_channels=None,\n",
    "                 norm='batch',\n",
    "                 act=\"relu\",\n",
    "                 dim_channel_multiplier=DIM_CHANNEL_MULTIPLIER):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dim_channel_multiplier = dim_channel_multiplier\n",
    "        self.norm = norm\n",
    "        self.act = act\n",
    "        self.max_channels = max_channels if max_channels else self.out_dim * self.dim_channel_multiplier\n",
    "        \n",
    "        # decide appropriate number of upsampling process based on expected output image shape\n",
    "        self.n_upsamples = get_n_samplings(self.out_dim)\n",
    "        \n",
    "        # projected to spatial extent convolutional repr. with feature maps\n",
    "        # x.shape == (batch_size, latent_dim)\n",
    "        layers = [\n",
    "            UnsqueezeLatent(),\n",
    "            UpsampleConv2d(in_channels=self.latent_dim,\n",
    "                           out_channels=self.max_channels,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           stride=1,  # no need to stride in first layer\n",
    "                           padding=0,  # no padding in first layer\n",
    "                           norm=self.norm,\n",
    "                           act=self.act)]\n",
    "        \n",
    "        # upsamples\n",
    "        # x.shape == (batch_size, max_channels, kernel_size, kernel_size)\n",
    "        chs = [self.max_channels // (2 ** i) for i in range(self.n_upsamples)]\n",
    "        chs.append(self.out_channels)\n",
    "        \n",
    "        layers.extend([\n",
    "            UpsampleConv2d(in_channels=in_ch,\n",
    "                           out_channels=out_ch,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           stride=2,\n",
    "                           norm=self.norm if i != self.n_upsamples else \"none\",\n",
    "                           act=self.act if i != self.n_upsamples else \"tanh\",\n",
    "                           bias=False if i != self.n_upsamples else True)\n",
    "         for i, (in_ch, out_ch) in enumerate(pairwise(chs), 1)])\n",
    "        # out.shape == (batch_size, out_channels, out_dim, out_dim)\n",
    "        \n",
    "        # final act: tanh\n",
    "        # using a bounded activation allowed the model to learn more quickly to \n",
    "        # saturate and cover the color space of the training distribution. \n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvGenerator(\n",
       "  (0): UnsqueezeLatent()\n",
       "  (1): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (2): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (3): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (4): UpsampleConv2d(\n",
       "    (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "for latent_dim, out_dim, out_ch in zip([128, 50, 100], [128, 64, 32], [3, 3, 1]):\n",
    "    x = torch.randn(batch_size, latent_dim)\n",
    "    g = ConvGenerator(latent_dim=latent_dim, out_dim=out_dim, out_channels=out_ch)\n",
    "    out = g(x)\n",
    "    assert out.shape == (batch_size, out_ch, out_dim, out_dim)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDiscriminator(nn.Sequential):\n",
    "    \"\"\"將特定大小圖片下採樣的辨識器\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 in_dim=32, \n",
    "                 norm=\"batch\",\n",
    "                 kernel_size=KERNEL_SIZE,\n",
    "                 max_channels=None,\n",
    "                 dim_channel_multiplier=DIM_CHANNEL_MULTIPLIER):\n",
    "        self.in_channels = in_channels\n",
    "        self.in_dim = in_dim\n",
    "        self.norm = norm\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_downsamples = get_n_samplings(self.in_dim)\n",
    "        self.dim_channel_multiplier = dim_channel_multiplier\n",
    "        self.max_channels = max_channels if max_channels else self.in_dim * self.dim_channel_multiplier\n",
    "        \n",
    "        # downsample\n",
    "        chs = [self.in_channels]\n",
    "        chs += sorted([self.max_channels // (2 ** i) for i in range(self.n_downsamples)])\n",
    "        \n",
    "        # x.shape == (batch_size, in_channels, in_dim, in_dim)\n",
    "        layers = [\n",
    "            DownsampleConv2d(in_ch, \n",
    "                             out_ch, \n",
    "                             self.kernel_size, \n",
    "                             stride=2, \n",
    "                             norm=self.norm if i != 1 else \"none\",\n",
    "                             bias=False if i != 1 else True)\n",
    "            for i, (in_ch, out_ch) in enumerate(pairwise(chs), 1)]\n",
    "        \n",
    "        # compute logits\n",
    "        # x.shape == (batch_size, max_channels, kernel_size, kernel_size)\n",
    "        layers.extend([\n",
    "            nn.Conv2d(chs[-1], 1, kernel_size=self.kernel_size),\n",
    "            SqueezeLogit()\n",
    "        ])\n",
    "        # out.shape == (batch_size, 1)\n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvDiscriminator(\n",
       "  (0): DownsampleConv2d(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (1): DownsampleConv2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (2): DownsampleConv2d(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (3): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (4): SqueezeLogit()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "for in_ch, in_dim in zip([3, 3, 1], [128, 64, 32]):\n",
    "    x = torch.randn(batch_size, in_ch, in_dim, in_dim)\n",
    "    d = ConvDiscriminator(in_ch, in_dim)\n",
    "    out = d(x)\n",
    "    assert out.shape == (batch_size, 1)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from practical_ai.data import get_data_loader, get_dataset\n",
    "# dataset = get_dataset(\"mnist\", split=\"train\")\n",
    "# data_loader = get_data_loader(dataset, batch_size=4)\n",
    "\n",
    "\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# import torchvision\n",
    "\n",
    "# # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "# writer = SummaryWriter('runs/test')\n",
    "\n",
    "# dataiter = iter(data_loader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# img_grid = torchvision.utils.make_grid(images)\n",
    "# writer.add_image('four_mnist_images', img_grid)\n",
    "\n",
    "# images.shape\n",
    "\n",
    "# z = torch.randn(1, latent_dim, 1, 1)\n",
    "\n",
    "# writer.add_graph(g, z)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "notebook2script()\n",
    "clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_practical_ai",
   "language": "python",
   "name": "conda_practical_ai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
